
Week1->Setting up your achine Learning Application->Basic Recipe for Machine Learning:
视频2

     |------------------------------------------------------------------\
    \|/                                                                 |
  High bias?                     =============>  Bigger network         |
 (training data performance)         Y          Train longer            |
    |                                           (New architecture search)
    |<------------------------------------------------------------------\
   \|/   N                                                              |
  High variance ?           =============>   More data                  |
 （dev set performance)             Y          Regularization
    |                                          (New architecture search)
   \|/  N
    Done

  要判断是否存在高偏差，你实际上就是要看模型在试练集上的表现:
如果说模型有高偏差，即模型甚至连训练集都不能良好拟合，你能尝试的一种办法是挑选一个新的网络，比如带有更多隐藏层或更多隐藏单元的
或是延长训练时间，让梯度下降法运行更长时间 或换用一些更高级的优化算法。另一个可以尝试的办法（这个办法可能有效 也可能无效），
但之后总归会看到，因为神经网络的结构有许多种，所以你能够找到一种更加适合当前问题的结构。我把它写在括号里 是因为它是一种需要你亲自
尝试的方法。也许最终你能使它有效，也许不能。相比起来，使用更大的网络几乎总是有效的。而延长训练时间 虽然并不永远有用，但是当然了不会
造成坏处。所以当训练一个学习算法时，我会尝试这些办法，直到我把偏差问题消除。所以我尝试完会回到这里然后再重复尝试。直到至少能良好地
拟合训练集。通常如果你的网络够大，应该就能够将训练集拟合好。只要这个学习问题是人类能完成的，对吧。如果图片非常模糊，也许就不可能拟合。
但如果至少人类能够良好地完成这个任务，如果你认为贝叶斯误差不是太大的话，那只要训练一个足够大的网络，就应当能够 或许应当能够在
训练集上取得良好的表现。也就是把训练集拟合 或是过拟合。
   当把偏差减小到可以接受的范围之后，就再问：这个问题是否存在高方差？
   要判断这一点，我会看模型在开发集上的表现。看模型是否具备一般化的能力，如在训练集上性能良好，当一般化到开发集时仍然性能较好？
如果你有比较高的方差，解决高方差问题的最好方法是取得更多数据——当然前提是你能获取得到，这个办法才有用。但有时你无法获得更多数据，
你还可以尝试正则化，这是我们下一节课中会讨论的。用它可以减少过拟合。还有一种，也需要你亲自尝试的方法：就是如果你能找到更合适的
神经网络结构，有时也能够在缓解方差问题的同时也缓解偏差问题。但具体怎么做呢？这里倒是不太容易总结出完全系统性的规律。
所以我也尝试这些办法，完后也回到初始点。直到找到一种低偏差、低方差的网络。
  这里有几点要注意的是：
  首先，依据你问题的不同，在高偏差和高方差时，你应当尝试的办法有可能很不一样。所以我通常用训练/开发集判断。
  然后根据这个来选择一些应当尝试的办法。举例来说，如果你有高偏差问题，就算取得更多训练数据也无济于事——至少也不是最有效的办法。
所以明确认识到是更像高偏差问题 或是高方差问题，或是二者皆备，就能帮助你选择最有用的办法。
  另外，在早些时代的机器学习中，有许多关于偏差和方差之间的取舍的讨论。这讨论存在的原因是，对于很多你能尝试的办法来说，
你只能在增大偏差的同时减小方差，或减小偏差的同时增大方差。但是尝试学习之前的时代中，我们能用的工具不是很多，我们没有大多
那种能够单独减小偏差或单独减小方差而不顾此失彼的工具。但在当前这个尝试学习和大数据的时代，只要你能不断扩大所训练的网络的规模，
只要你能不断获得更多数据，虽然这两点都不是永远成立的，但如果这两点是可能的，那扩大网络几乎总是能够减小偏差而不增大方差。
只要你用恰当的方式正则化的话，而获得更多数据几乎问题能够减小方差而不增大偏差。所以归根结底，有这这两步以后，再加上能够选取
不同的网络来训练，以及获取更多数据的能力，我们就有了能够且只单独削减偏差，同时不会过多影响另一个指标的能力。它能够解释为何
深度学习在监督学习中如果有用，以及为何在深度学习中偏差与方差的权衡要不明显得多。这样你就不需小心地平衡两者，而是因为有了更多选择
可以单独削减偏差 或者单独削减方差 而不会同时增加方差或偏差。而且事实上当你有了一个良好的正则化的网络时，我们将在下一节课中讨论正则化。
训练一个更大的网络几乎从来没有坏处，当训练的神经网络太大时主要的代价只是计算时间，只要你采取正则化就行。
  我希望这个视频能给你一些基本的概念，知道如何有条理地诊断机器学习中的偏差与方差问题。然后采取正确的办法来在问题中取得进展。
我在视频中多次提到过正则化的概念：它是用于减小方差的一个很有用的办法。在正则化中存在一点点偏差与方差间的权衡，它可能会使
偏差增加一点点——虽然在你的网络足够巨大时 增加得通常不会很多。所以让我们在下一下视频中深入讨论一下如何对神经网络进行正则化。
