{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA INGESTION AND EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.stop()\n",
    "sc = SparkContext('local')\n",
    "pyspark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../../datasets/data_sf-airbnb/sf-airbnb-clean.parquet/\"\n",
    "airbnb_df = pyspark.read.parquet(file_path)\n",
    "airbnb_df.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\n",
    "                \"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will predict the price per night for rental property with the given features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE TRAINING AND TESTS DATASETS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We determine test/train size according to the size of the dataset. The dataset size varies and hence the train/test split also varies.\n",
    "- in this case we will split the dataset into 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: 5780\n",
      "test_df: 1366\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = airbnb_df.randomSplit([.8, .2], seed=seed)\n",
    "print(\"train_df: {}\".format(train_df.count()))\n",
    "print(\"test_df: {}\".format(test_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark **Catalyst Optimizer** determines the optimal way to partition data as a function of the cluster resources and size of data set. Data in Spark is row partitioned and each worker split independently, if data in partitions changes then random split won't give us the same data as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEST PRACTICE:** Since we use training data again and again, its best to cache the trainig data for faster and better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Features with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting price with linear regression, linear regression in spark requires all input features to be in a single vector in the dataframe so we will transform the data as per the need.<br>\n",
    "**VectorAssembler transformer**  takes input list columns and create a new Dataframe with additional columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vec_assembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
    "vec_train_df = vec_assembler.transform(train_df)\n",
    "vec_train_df.select(\"bedrooms\", \"features\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have multiple linear regression in which there are multiple independent values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using estimators to build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lr_model = lr.fit(vec_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr_model is a transformer, the output of an estimators fit() method is transformer. Once the transformer has learned the parameter, the transformer can apply these parameters to new data points to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m,b 123.68,47.51\n"
     ]
    }
   ],
   "source": [
    "m = round(lr_model.coefficients[0], 2)\n",
    "b = round(lr_model.intercept, 2)\n",
    "print(\"m,b {},{}\".format(m,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we want to apply our model to our test set, then we need to prepare that data in the same way as the training set (i.e., pass it through the vector assembler).**\n",
    "- Oftentimes data preparation pipelines will have multiple steps, and it becomes cumbersome to remember not only which steps to apply, but also the ordering of the steps.<br>\n",
    "- This is the motivation for the Pipeline API: you simply specify the stages you want your data to pass through, in order, and Spark takes care of the processing for you.\n",
    "- **They provide the user with better code reusability and organization. In Spark, Pipelines are estimators, whereas PipelineModels—fitted Pipelines—are transformers.**\n",
    "\n",
    "**NOTE IN SIMPLE TERMS:** Pipeline makes the code organized as the code has been run randomly before (esp in the notebooks), for making and tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vec_assembler, lr])\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pipeline model is transformer it is straightforward to appy test data to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+------------------+\n",
      "|bedrooms|features| price|        prediction|\n",
      "+--------+--------+------+------------------+\n",
      "|     1.0|   [1.0]|  85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 159.0|171.18598011578285|\n",
      "|     2.0|   [2.0]| 250.0|294.86172649777757|\n",
      "|     1.0|   [1.0]|  99.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  95.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 100.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|2010.0|171.18598011578285|\n",
      "+--------+--------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_df.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  NOW BUILDING A MULTI FEATURE MODEL PIPELINE FOR THE SAME PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we will transform the categorical columns with one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Dog\" = [ 1, 0, 0]<br>\n",
    "\"Cat\" = [ 0, 1, 0]<br>\n",
    "\"Fish\" = [0, 0, 1]<br><br>If we had a zoo of 300 animals, would OHE massively increase consumption of memory/compute resources? Not with Spark! Spark internally uses a SparseVector when the majority of the entries are 0, as is often the case after OHE, so it does not waste space storing 0 values.<br><br>DenseVector(0, 0, 0, 7, 0, 2, 0, 0, 0, 0)<br>\n",
    "SparseVector(10, [3, 5], [7, 2])<br><br>The DenseVector in this example contains 10 values, all but 2 of which are 0. \n",
    "In SparceVector we only track the non-zero values by their index and values, rest are considered 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created our category indices, we can pass those input to OneHotEncoder (OneHotEncoderEstimator in Spark 2.3/2.4). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name-----Spark2.3/2.4-----Spark3.0**<br>\n",
    "StringIndexer-----Single column as input/output-----Multiple columns as input/output<br>\n",
    "OneHotEncoder-----Deprecated-----Multiple columns as input/output<br>\n",
    "OneHotEncoderEstimator-----Multiple columns as input/output-----N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the given probel we take any string type as a categorical feature, but sometimes we may numeric features which needs to be treated as categorical or vice versa. We must carefully determine which one is numeric and which ones are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "ohe_output_cols = [x + \"OHE\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols,\n",
    "                              outputCols=index_output_cols,\n",
    "                              handleInvalid=\"skip\")\n",
    "ohe_encoder = OneHotEncoder(inputCols=index_output_cols,\n",
    "                            outputCols=ohe_output_cols)\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assemble_inputs = ohe_output_cols + numeric_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=assemble_inputs,\n",
    "                               outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StringIndexer handle invalid data with handleInvalid parameter that specifies how to handle them. Options are:\n",
    "- skip\n",
    "- error\n",
    "- keep\n",
    "\n",
    "We need to explicitely tell StringIndexer which parameter to treat as Categorical. Another way is by using VectorIndexer which is computationally expensive as it goes through every record to find out distinct values, user can also define maxCategories but its hard to determine.\n",
    "\n",
    "<br>**Best approach** is **RFormula**<br>\n",
    "In RFormula we provide label and which features we want to include. It supports a limited subset of the R operators, including ~, ., :, +, and -. For example, you might specify formula = \"y ~ bedrooms + bathrooms\", which means to predict y given just bedrooms and bathrooms, or formula = \"y ~ .\", which means to use all of the available features (and automatically excludes y from the features). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFormula will automatically StringIndex and OHE all of your string columns, convert your numeric columns to double type, and combine all of these into a single vector using VectorAssembler under the hood. Thus, we can replace all of the preceding code with a single line, and we will get the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "r_formula = RFormula(formula=\"price ~ .\",\n",
    "                    featuresCol=\"features\",\n",
    "                    labelCol=\"price\",\n",
    "                    handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFormula downside \n",
    "The downside of RFormula automatically combining the StringIndexer and OneHotEncoder is that one-hot encoding is not required or recommended for all algorithms. For example, tree-based algorithms can handle categorical variables directly if you just use the StringIndexer for the categorical features. You do not need to one-hot encode categorical features for tree-based methods, and it will often make your tree-based models worse. Unfortunately, **there is no one-size-fits-all solution for feature engineering, and the ideal approach is closely related to the downstream algorithms** you plan to apply to your data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE\n",
    "If someone else performs the feature engineering for you, make sure they document how they generated those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will put feature preparation and model building into the pipeline and apply it to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,22,43,...| 85.0| 55.24365707389188|\n",
      "|(98,[0,3,6,22,43,...| 45.0|23.357685914717877|\n",
      "|(98,[0,3,6,22,43,...| 70.0|28.474464479034395|\n",
      "|(98,[0,3,6,12,42,...|128.0| -91.6079079594947|\n",
      "|(98,[0,3,6,12,43,...|159.0| 95.05688229945372|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages =  [string_indexer, ohe_encoder, vec_assembler, lr])\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "pred_df = pipeline_model.transform(test_df)\n",
    "pred_df.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the feature column is represented as a SparseVector. There are 98 features after one-hot encoding, followed by non-zero indices and the values themselves.<br>\n",
    "**In our predictions we have negative value, rent can never be negative, hence the model needs to be improved.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING MODELS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark we have classification, regression, clustering and ranking evaluators. Since its a regression problem therefore we will use root-mean-square (RMSE) and R squared to evaluate the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSE ranges from 0-infinity, the closer to infinity the better the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regression_evaluator = RegressionEvaluator(\n",
    "predictionCol = \"prediction\",\n",
    "labelCol=\"price\",\n",
    "metricName='rmse')\n",
    "rmse = regression_evaluator.evaluate(pred_df)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the value of RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we know if 220.6 is a good value for the RMSE? **There are various ways to interpret this value, one of which is to build a simple baseline model and compute its RMSE to compare against. A common baseline model for regression tasks is to compute the average value of the label on the training set ȳ (pronounced y-bar), then predict ȳ for every record in the test data set and compute the resulting RMSE** (its implemented elsewhere). If you try this, you will see that our baseline model has an RMSE of 240.7, so we beat our baseline. If you don’t beat the baseline, then something probably went wrong in your model building process.<br>\n",
    "**ROLE OF UNIT**<br>\n",
    "Keep in mind that the unit of your label directly impacts your RMSE. For example, if your label is height, then your RMSE will be higher if you use centimeters rather than meters as your unit of measurement. You could arbitrarily decrease the RMSE by using a different unit, which is why it is important to compare your RMSE against a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-squared another evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
