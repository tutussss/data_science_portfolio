{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wajeeh-machine/anaconda3/envs/mytf2/lib/python3.7/site-packages/pyspark/sql/context.py:77: DeprecationWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.stop()\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../datasets/data_sf-airbnb/sf-airbnb-clean.parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df = spark.read.parquet(file_path)\n",
    "(train_df, test_df) = airbnb_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [field for (field, dataType) in train_df.dtypes if dataType == \"string\"]\n",
    "\n",
    "index_output_cols = [x + \"Index\" for x in categorical_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=categorical_cols,\n",
    "                              outputCols=index_output_cols,\n",
    "                              handleInvalid=\"skip\")\n",
    "\n",
    "numeric_cols = [field for (field, dataType) in train_df.dtypes\n",
    "               if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "assembler_inputs = index_output_cols + numeric_cols\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=assembler_inputs,\n",
    "                               outputCol=\"features\")\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, maxDepth=5,\n",
    "                          numTrees=100, seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer, vec_assembler, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To start logging with MLFlow**, we need to start a run using mlflow.start_run(), we won't call mlflow.end_run(), the clause at the end will automatically end the block**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in example below pipeline is the SPARK PIPELINE, we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with mlflow.start_run(run_name=\"random-forest\") as run:\n",
    "    # log params: num_trees and max_depth\n",
    "    mlflow.log_param(\"num_trees\", rf.getNumTrees())\n",
    "    mlflow.log_param(\"max_depth\", rf.getMaxDepth())\n",
    "    \n",
    "    #Log model\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "    mlflow.spark.log_model(pipeline_model, \"model\")\n",
    "    \n",
    "    # Log metrics: RMSE and R2\n",
    "    # first get the pre from the model\n",
    "    pred_df = pipeline_model.transform(test_df)\n",
    "    \n",
    "    # create a regression evaluator R2 and RMSE\n",
    "    regression_evaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                              labelCol=\"price\")\n",
    "    rmse = regression_evaluator.setMetricName(\"rmse\").evaluate(pred_df)\n",
    "    r2 = regression_evaluator.setMetricName(\"r2\").evaluate(pred_df)\n",
    "    # now define a MLFlow metric \n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "    \n",
    "    # Log artefact: feature importance scores\n",
    "    rf_model = pipeline_model.stages[-1]\n",
    "    pandas_df = (pd.DataFrame(list(zip(vec_assembler.getInputCols(), \n",
    "                                    rf_model.featureImportances)), \n",
    "                           columns=[\"feature\", \"importance\"])\n",
    "              .sort_values(by=\"importance\", ascending=False))\n",
    "    \n",
    "    # First write to local filesystem then tell MLFlow where to \n",
    "    # find that file\n",
    "    pandas_df.to_csv(\"feature-importance.csv\", index=False)\n",
    "    mlflow.log_artifact(\"feature-importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can examine the MLFlow UI, which can be accessed by runing **mlflow ui** command in the terminal and navigating to **localhost:5000**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UI stores all the runs for a given experiment. **You can search across all the runs, filter for those that meet particular criteria, compare runs side by side, etc. If you wish, you can also export the contents as a CSV file to analyze locally.** Click on the run in the UI named \"random-forest\".<br>\n",
    "You’ll notice that it **keeps track of the source code used for this MLflow run, as well as storing all the corresponding parameters, metrics, etc. You can add notes about this run in free text, as well as tags. You cannot modify the parameters or metrics after the run has finished.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can also query the tracking server using the MlflowClient or REST API:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r2': 0.22794251914574226, 'rmse': 211.5096898777315}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "runs = client.search_runs(run.info.experiment_id,\n",
    "                         order_by=[\"attributes.start_time desc\"],\n",
    "                         max_results=1)\n",
    "run_id = runs[0].info.run_id\n",
    "runs[0].data.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROLE OF YAML FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** We have YAML file that defines which libraries are required to run the code so, it could be seaminglessly deployed on another machine/environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is the conda.yaml file for running this mlflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda.yaml file\n",
    "\n",
    "name: mlflow-project-example\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.7\n",
    "  - pandas=0.24\n",
    "  - pip=19.0.3\n",
    "  - pip:\n",
    "    - mlflow==1.8\n",
    "    - pyspark==3.0.0\n",
    "\n",
    "we can run the project having proper file folder structure and code, as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**below statement runs perfectly but we don't need to run it right now**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mlflow.run(\n",
    "  \"https://github.com/databricks/LearningSparkV2/#mlflow-project-example\", \n",
    "  parameters={\"max_depth\": 5, \"num_trees\": 100})**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we can run mlflow from command line as well** by issuing the following command:<br><br>\n",
    "**mlflow run https://github.com/databricks/LearningSparkV2/#mlflow-project-example\n",
    "-P max_depth=5 -P num_trees=100**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment Option With MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning model deployment have different meaning for every organization and use case. Business constraints will different requirements for latency, throughput,cost etc.\n",
    "Throughput and latency has trade-off for different deployment options for generating predictions. We car about both concurrent requests and the size of those requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\;\\;$$\\;\\;$$\\;\\;$$\\;\\;$$\\;\\;$ | Throughput$\\;\\;$L| atency$\\;\\;$| Example application**<br>\n",
    "Batch$\\;\\;$$\\;\\;$$\\;\\;$| High$\\;\\;$High (hours to days)| $\\;\\;$Customer churn prediction<br>\n",
    "Streaming$\\;\\;$Medium$\\;\\;| $Medium (seconds to minutes)| $\\;\\;$Dynamic pricing<br>\n",
    "Real-time$\\;\\;$Low$\\;\\;$| Low (milliseconds)$\\;\\;$| Online ad bidding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch processing**<br>\n",
    "- Batch processing generates predictions on a regular schedule and writes the result out to persistent storage to be served elsewhere. \n",
    "- Cheapest and easiest deployment option as we only pay for the compute during scheduled run.\n",
    "- Much efficient per data point.\n",
    "- **DRAWBACK** its main drawback is latency, as it is typically scheduled with a period of hours or days to generate the next batch of predictions.\n",
    "\n",
    "**Streaming processing**\n",
    "- Streaming provides a nice trade-off between throughput and latency.\n",
    "- You will continuously make predictions on micro-batches of data and get your predictions in seconds to minutes.\n",
    "- If you are using Structured Streaming, almost all of your code will look identical to the batch use case, making it easy to go back and forth between these two options. \n",
    "- **DRAWBACK** With streaming, you will have to pay for the VMs or computing resources you use to continually stay up and running, and ensure that you have configured the stream properly to be fault tolerant and provide buffering if there are spikes in the incoming data.\n",
    "\n",
    "**Realtime processing**\n",
    "- Real-time deployment prioritizes latency over throughput and generates predictions in a few milliseconds. \n",
    "- Your **infrastructure will need to support load balancing** and be able to scale to many concurrent requests if there is a large spike in demand (e.g., for online retailers around the holidays).\n",
    "- **Sometimes when people say “real-time deployment” they mean extracting precomputed predictions in real time, but here we’re referring to generating model predictions in real time.**\n",
    "- **Real-time deployment is the only option that Spark cannot meet the latency requirements for, so to use it you will need to export your model outside of Spark.** \n",
    "- **DRAWBACK of Spark** For example, if you intend to use a REST endpoint for real-time model inference (say, computing predictions in under 50 ms), **MLlib does not meet the latency requirements necessary for this application**. You will need to get your feature preparation and model out of Spark, which can be time-consuming and difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin the modeling process, you need to define your model deployment requirements. MLlib and Spark are just a few tools in your toolbox, and you need to understand when and where they should be applied. The remainder of this section discusses the deployment options for MLlib in more depth, and then we’ll consider the deployment options with Spark for non-MLlib models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BATCH DEPLOYMENT\n",
    "- Handles majority of use case and easiest\n",
    "- we run regular job to generate predictions and save the results to a table, database, data lake, etc, for downstream consumption.\n",
    "- we already did batch preictions before by using .transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/07/16 05:22:42 INFO mlflow.spark: 'runs:/5ae37bc3587241e39835bd937f011264/model' resolved as 'file:///home/wajeeh-machine/projects/data_science_portfolio/machine_learning_pipeline/notebooks/mlruns/0/5ae37bc3587241e39835bd937f011264/artifacts/model'\n",
      "2020/07/16 05:22:42 INFO mlflow.spark: File 'file:///home/wajeeh-machine/projects/data_science_portfolio/machine_learning_pipeline/notebooks/mlruns/0/5ae37bc3587241e39835bd937f011264/artifacts/model/sparkml' is already on DFS, copy is not necessary.\n"
     ]
    }
   ],
   "source": [
    "import mlflow.spark\n",
    "pipeline_model = mlflow.spark.load_model(f\"runs:/{run_id}/model\")\n",
    "\n",
    "# Generate predictions\n",
    "input_df = spark.read.parquet(\"../../datasets/data_sf-airbnb/sf-airbnb-clean.parquet/\")\n",
    "pred_df = pipeline_model.transform(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important for batch deployment**<br>\n",
    "- How frequently will you generate predictions?\n",
    "    - There is a trade-off between latency and throughput. \n",
    "    - You will get higher throughput batching many predictions together, but then the time it takes to receive any individual predictions will be much longer, delaying your ability to act on these predictions.\n",
    "- How often will you retrain the model?\n",
    "    - Unlike libraries like sklearn or TensorFlow, MLlib does not support online updates or warm starts. **If you’d like to retrain your model to incorporate the latest data, you’ll have to retrain the entire model from scratch, rather than getting to leverage the existing parameters.**\n",
    "    - **In terms of the frequency of retraining, some people will set up a regular job to retrain the model (e.g., once a month), while others will actively monitor the model drift to identify when they need to retrain.**\n",
    "- How will you version the model?\n",
    "    - You can use the **MLflow Model Registry** to keep track of the models you are using and **control how they are transitioned to/from staging, production, and archived.** You can use the Model Registry with the other deployment options too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to using the MLflow UI to manage your models, you can also manage them programmatically. For example, once you have registered your production model, it has a consistent URI that you can use to retrieve the latest version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"random-forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MlflowException",
     "evalue": "Model Registry features are not supported by the store with URI: 'file:///home/wajeeh-machine/projects/data_science_portfolio/machine_learning_pipeline/notebooks/mlruns'. Stores with the following URI schemes are supported: ['databricks', 'http', 'https', 'postgresql', 'mysql', 'sqlite', 'mssql'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/mlflow/tracking/registry.py\u001b[0m in \u001b[0;36mget_store_builder\u001b[0;34m(self, store_uri)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mstore_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnsupportedModelRegistryStoreURIException\u001b[0m Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36m_get_registry_client\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mregistry_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelRegistryClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registry_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# Define an instance variable on this `MlflowClient` instance to reference the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/mlflow/tracking/_model_registry/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, registry_uri)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/mlflow/tracking/_model_registry/utils.py\u001b[0m in \u001b[0;36m_get_store\u001b[0;34m(store_uri)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_model_registry_store_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/mlflow/tracking/_model_registry/registry.py\u001b[0m in \u001b[0;36mget_store\u001b[0;34m(self, store_uri)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mstore_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_uri\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstore_uri\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tracking_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_store_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/mlflow/tracking/registry.py\u001b[0m in \u001b[0;36mget_store_builder\u001b[0;34m(self, store_uri)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0munsupported_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstore_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 supported_uri_schemes=list(self._registry.keys()))\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstore_builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnsupportedModelRegistryStoreURIException\u001b[0m:  Model registry functionality is unavailable; got unsupported URI 'file:///home/wajeeh-machine/projects/data_science_portfolio/machine_learning_pipeline/notebooks/mlruns' for model registry data storage. Supported URI schemes are: ['databricks', 'http', 'https', 'postgresql', 'mysql', 'sqlite', 'mssql']. See https://www.mlflow.org/docs/latest/tracking.html#storage for how to run an MLflow server against one of the supported backend storage locations.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4c7b587ab5bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_production_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mF\"models:/{model_name}/production\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_production\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_production_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/mlflow/spark.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_uri, dfs_tmpdir)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mModelsArtifactRepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_models_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mruns_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0mmodel_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelsArtifactRepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_underlying_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'%s' resolved as '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruns_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0mflavor_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_flavor_configuration_from_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAVOR_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/mlflow/store/artifact/models_artifact_repo.py\u001b[0m in \u001b[0;36mget_underlying_uri\u001b[0;34m(uri)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelsArtifactRepository\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mlatest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_latest_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 raise MlflowException(\"No versions of model with name '{name}' and \"\n",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36mget_latest_versions\u001b[0;34m(self, name, stages)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelVersion\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_registry_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_latest_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# Model Version Methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36m_get_registry_client\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0;34m\" '{uri}'. Stores with the following URI schemes are supported:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0;34m\" {schemes}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registry_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschemes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_uri_schemes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                     FEATURE_DISABLED)\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mregistry_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMlflowException\u001b[0m: Model Registry features are not supported by the store with URI: 'file:///home/wajeeh-machine/projects/data_science_portfolio/machine_learning_pipeline/notebooks/mlruns'. Stores with the following URI schemes are supported: ['databricks', 'http', 'https', 'postgresql', 'mysql', 'sqlite', 'mssql']."
     ]
    }
   ],
   "source": [
    "model_production_uri = F\"models:/{model_name}/production\"\n",
    "model_production = mlflow.spark.load_model(model_production_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STREAMING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of waiting for an hourly or nightly job to process your data and generate predictions, Structured Streaming can continuously perform inference on incoming data.\n",
    "- While this approach is more costly than a batch solution as you have to continually pay for compute time (and get lower throughput), you get the added benefit of generating predictions more frequently so you can act on them sooner.\n",
    "- Streaming solutions in general are more complicated to maintain and monitor than batch solutions, but they offer lower latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Spark we only need to use spark.readStream() instead of stream.read() to convert batch predictions to streaming predictions.**\n",
    "We will define a schema even if we are using parquet files b/c we define schema before working with streaming predictions/data. <br>\n",
    "We will utilize random forest model saved in previous example. **We will load using MLFlow.** We have partitioned the source file into one hundred small Parquet files so you can see the output changing at every trigger interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/07/16 05:55:53 INFO mlflow.spark: 'runs:/5ae37bc3587241e39835bd937f011264/model' resolved as 'file:///home/wajeeh-machine/projects/data_science_portfolio/machine_learning_pipeline/notebooks/mlruns/0/5ae37bc3587241e39835bd937f011264/artifacts/model'\n",
      "2020/07/16 05:55:53 INFO mlflow.spark: File 'file:///home/wajeeh-machine/projects/data_science_portfolio/machine_learning_pipeline/notebooks/mlruns/0/5ae37bc3587241e39835bd937f011264/artifacts/model/sparkml' is already on DFS, copy is not necessary.\n"
     ]
    }
   ],
   "source": [
    "# load saved model with MLFLow\n",
    "pipeline_model = mlflow.spark.load_model(f\"runs:/{run_id}/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a parquet file for streaming data in local directory with 100 partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we don't have 100p partitioned data right now, we will check below code chunk later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/wajeeh-machine/projects/data_science_portfolio/datasets/data_sf-airbnb/sf-airbnb-clean-100p.parquet;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2c2920e61487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# setup simulated streaming data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrepartitioned_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../datasets/data_sf-airbnb/sf-airbnb-clean-100p.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepartitioned_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m streaming_data = (spark\n",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    351\u001b[0m         self._set_opts(mergeSchema=mergeSchema, pathGlobFilter=pathGlobFilter,\n\u001b[1;32m    352\u001b[0m                        recursiveFileLookup=recursiveFileLookup)\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mytf2/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/wajeeh-machine/projects/data_science_portfolio/datasets/data_sf-airbnb/sf-airbnb-clean-100p.parquet;"
     ]
    }
   ],
   "source": [
    "# setup simulated streaming data\n",
    "repartitioned_path = \"../../datasets/data_sf-airbnb/sf-airbnb-clean-100p.parquet\"\n",
    "schema = spark.read.parquet(repartitioned_path).schema\n",
    "\n",
    "streaming_data = (spark\n",
    "                 .readStream\n",
    "                 .schema(schema) # Can set the schema this way\n",
    "                 .option(\"maxFilesPerTrigger\", 1)\n",
    "                 .parquet(repartitioned_path))\n",
    "\n",
    "# Generate predictions\n",
    "streamed_pred = pipeline_model.transform(streaming_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Export Patterns For Real-Time Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are some domains where real-time inference is required, including fraud detection, ad recommendation, and the like. While making predictions with a small number of records may achieve the low latency required for real-time inference, **you will need to contend with load balancing (handling many concurrent requests) as well as geolocation in latency-critical tasks**. \n",
    "- **There are popular managed solutions, such as AWS SageMaker and Azure ML, that provide low-latency model serving solutions.** In this section we’ll show you how to export your MLlib models so they can be deployed to those services.\n",
    "- **One way to export your model out of Spark is to reimplement the model natively in Python, C, etc.** While it may seem simple to extract the coefficients of the model, exporting all the feature engineering and preprocessing steps along with them (OneHotEncoder, VectorAssembler, etc.) **quickly gets troublesome and is very error-prone.** \n",
    "- *There are a few open source libraries, such as MLeap and ONNX, that can help you automatically export a supported subset of the MLlib models to remove their dependency on Spark. However, as of the time of this writing the company that developed MLeap is no longer supporting it. Nor does MLeap yet support Scala 2.12/Spark 3.0.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ONYX\n",
    "ONNX (Open Neural Network Exchange), on the other hand, has become the de facto open standard for machine learning interoperability. Some of you might recall other ML interoperability formats, like PMML (Predictive Model Markup Language), but those never gained quite the same traction as ONNX has now. ONNX is very popular in the deep learning community as a tool that allows developers to easily switch between libraries and languages, and at the time of this writing it has experimental support for MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### #rd party libraries for exporting models\n",
    "There are other third-party libraries that integrate with Spark that are convenient to deploy in real-time scenarios, such as XGBoost and H2O.ai’s Sparkling Water (whose name is derived from a combination of H2O and Spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost advantages\n",
    "- **XGBoost is one of the most successful algorithms in Kaggle competitions for structured data problems**, and it’s a very popular library among data scientists.\n",
    "- Although XGBoost is not technically part of MLlib, the XGBoost4J-Spark library allows you to integrate distributed XGBoost into your MLlib pipelines.\n",
    "- **A benefit of XGBoost is the ease of deployment: after you train your MLlib pipeline, you can extract the XGBoost model and save it as a non-Spark model for serving in Python**, as demonstrated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/wajeeh-machine/anaconda3/envs/mytf2/lib/python3.7/site-packages (1.1.1)\r\n",
      "Requirement already satisfied: numpy in /home/wajeeh-machine/anaconda3/envs/mytf2/lib/python3.7/site-packages (from xgboost) (1.18.5)\r\n",
      "Requirement already satisfied: scipy in /home/wajeeh-machine/anaconda3/envs/mytf2/lib/python3.7/site-packages (from xgboost) (1.5.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  loading an XGBoost model is as simple as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "bst = xgb.Booster({'nthread': 4})\n",
    "bst.load_model(\"xgboost_native_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
