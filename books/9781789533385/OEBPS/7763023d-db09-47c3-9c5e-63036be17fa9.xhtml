<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis" xmlns:svg="http://www.w3.org/2000/svg">
<head>
  <meta charset="UTF-8" />
  <title>Working with big data</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css" />
  <link type="text/css" rel="stylesheet" media="all" href="core.css" />
</head>
<body>
  <div id="sbo-rt-content"><section>

    <header>

      <h1 class="header-title">Working with big data</h1>

    </header>

    <article>

      <p>Tableau Prep works with big data volumes and big data tools, such as Snowflake, Redshift, and Amazon EMR. You can refer to <a href="03d6a7f8-eb7e-4966-8b99-da1fff93692c.xhtml">Chapter 10</a>, <em>Tableau for Big Data</em> and connect existing accounts of Amazon Redshift, Amazon EMR, or Snowflake.</p>

      <p>Tableau Prep allows us to work with big data sets by leveraging sampling. However, it processes data on your local machine and if you want to create, extract, or export data into a CSV using a huge dataset, it can fail due to lack of memory. We learned that Tableau Desktop works with big data by rendering results using a live connection. We don't want to create an extract when working with big data. In the case of Tableau Prep, we can learn our dataset and then use filters to split the dataset and work with part of it.</p>

      <div class="packt_tip">Here's another solution: we can launch a powerful AWS EC2 instance and install Tableau Prep there, where it will use more resources.</div>

    </article>

  </section>

</div>
</body>
</html>