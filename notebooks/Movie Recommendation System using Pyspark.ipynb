{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### download dataset from here \n",
    "! wget http://files.grouplens.org/datasets/movielens/ml-25m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genome-scores.csv  links.csv   ratings.csv  tags.csv\r\n",
      "genome-tags.csv    movies.csv  README.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../datasets/movie-ml-25/ml-25m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext('local', 'movie recommendation system')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.5\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../datasets/movie-ml-25/ml-25m/ratings.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    296|   5.0|1147880044|\n",
      "|     1|    306|   3.5|1147868817|\n",
      "|     1|    307|   5.0|1147868828|\n",
      "|     1|    665|   5.0|1147878820|\n",
      "|     1|    899|   3.5|1147868510|\n",
      "|     1|   1088|   4.0|1147868495|\n",
      "|     1|   1175|   3.5|1147868826|\n",
      "|     1|   1217|   3.5|1147878326|\n",
      "|     1|   1237|   5.0|1147868839|\n",
      "|     1|   1250|   4.0|1147868414|\n",
      "|     1|   1260|   3.5|1147877857|\n",
      "|     1|   1653|   4.0|1147868097|\n",
      "|     1|   2011|   2.5|1147868079|\n",
      "|     1|   2012|   2.5|1147868068|\n",
      "|     1|   2068|   2.5|1147869044|\n",
      "|     1|   2161|   3.5|1147868609|\n",
      "|     1|   2351|   4.5|1147877957|\n",
      "|     1|   2573|   4.0|1147878923|\n",
      "|     1|   2632|   5.0|1147878248|\n",
      "|     1|   2692|   5.0|1147869100|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data , test_data = df.randomSplit([0.7,0.3],random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "        maxIter=15,\n",
    "        rank=10,\n",
    "        seed=1234,\n",
    "        ratingCol=('rating'),\n",
    "        userCol='userId',\n",
    "        itemCol='movieId'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17502939"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7497156"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = als.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- prediction: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|prediction|\n",
      "+------+-------+------+----------+----------+\n",
      "| 13403|   2772|   2.0| 993737923| 3.5786314|\n",
      "|157541|  71535|   4.0|1443476967| 3.6844072|\n",
      "|162296|   3248|   3.5|1443129982| 3.0449328|\n",
      "|154050|  43560|   3.5|1419614232|  3.370797|\n",
      "|  8408|  74228|   4.0|1532394392| 3.6625733|\n",
      "| 72315| 185723|   2.5|1551180065| 1.4301612|\n",
      "| 59145|   1580|   5.0|1026178441| 3.6775548|\n",
      "| 68341|   6936|   4.5|1564364156| 3.5418048|\n",
      "| 59756|   2021|   2.0|1281384322| 3.1852303|\n",
      "|139907|    454|   3.0| 846515944|  2.993246|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|prediction|\n",
      "+------+-------+------+----------+----------+\n",
      "|114572|    148|   2.0| 838460783| 2.4473324|\n",
      "|159730|    148|   3.0| 842162037| 2.7568665|\n",
      "| 47989|    148|   2.0| 833173771| 3.1970022|\n",
      "| 72337|    148|   2.0| 944246202| 2.8859456|\n",
      "|108767|    148|   3.0|1276969740|  2.621679|\n",
      "| 21531|    148|   3.0| 834035555|  3.017579|\n",
      "| 99684|    148|   3.0|1027645782| 2.9729304|\n",
      "| 35969|    148|   2.0| 835094487| 2.9234517|\n",
      "| 29943|    148|   3.0|1049216998| 2.9995558|\n",
      "|117168|    148|   4.0| 835820190| 2.9894955|\n",
      "|  3411|    148|   3.0| 835966104| 2.7953959|\n",
      "| 28229|    148|   1.0| 833850593| 2.6176789|\n",
      "|148197|    148|   2.5|1207008368| 2.8564281|\n",
      "|  6491|    148|   4.0|1500217059| 2.6247742|\n",
      "|147301|    148|   3.0| 951070210| 2.7431169|\n",
      "|111567|    148|   3.0| 945399307| 2.9379678|\n",
      "| 98520|    148|   4.0|1034547175| 2.8708107|\n",
      "| 73827|    148|   4.0|1490671894| 2.8109019|\n",
      "| 66440|    148|   2.5|1099143605| 3.0342934|\n",
      "|145182|    148|   3.0| 944952722| 2.7149987|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION AND PREDICTIONS ON TEST DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse', \n",
    "                                predictionCol='prediction',\n",
    "                                labelCol='rating')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend top movies that active user might like "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our predictions have Nan and we can use coldstratstrategy to handle the issue later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = predictions.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------+\n",
      "|userId|movieId|rating| timestamp|prediction|\n",
      "+------+-------+------+----------+----------+\n",
      "|114572|    148|   2.0| 838460783| 2.4473324|\n",
      "|159730|    148|   3.0| 842162037| 2.7568665|\n",
      "| 47989|    148|   2.0| 833173771| 3.1970022|\n",
      "| 72337|    148|   2.0| 944246202| 2.8859456|\n",
      "|108767|    148|   3.0|1276969740|  2.621679|\n",
      "| 21531|    148|   3.0| 834035555|  3.017579|\n",
      "| 99684|    148|   3.0|1027645782| 2.9729304|\n",
      "| 35969|    148|   2.0| 835094487| 2.9234517|\n",
      "| 29943|    148|   3.0|1049216998| 2.9995558|\n",
      "|117168|    148|   4.0| 835820190| 2.9894955|\n",
      "|  3411|    148|   3.0| 835966104| 2.7953959|\n",
      "| 28229|    148|   1.0| 833850593| 2.6176789|\n",
      "|148197|    148|   2.5|1207008368| 2.8564281|\n",
      "|  6491|    148|   4.0|1500217059| 2.6247742|\n",
      "|147301|    148|   3.0| 951070210| 2.7431169|\n",
      "|111567|    148|   3.0| 945399307| 2.9379678|\n",
      "| 98520|    148|   4.0|1034547175| 2.8708107|\n",
      "| 73827|    148|   4.0|1490671894| 2.8109019|\n",
      "| 66440|    148|   2.5|1099143605| 3.0342934|\n",
      "|145182|    148|   3.0| 944952722| 2.7149987|\n",
      "+------+-------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8048781346451978\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse', \n",
    "                                predictionCol='prediction',\n",
    "                                labelCol='rating')\n",
    "rmse = evaluator.evaluate(predictions2)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating recommendations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------------+\n",
      "|userId|recommendations                                               |\n",
      "+------+--------------------------------------------------------------+\n",
      "|148   |[[183947, 5.505346], [203086, 5.443754], [184299, 5.287776]]  |\n",
      "|463   |[[185959, 6.594113], [176597, 6.4512553], [184299, 6.3820105]]|\n",
      "|471   |[[176597, 5.7674417], [204302, 5.541669], [173153, 5.3972287]]|\n",
      "+------+--------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.recommendForAllUsers(3).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------+\n",
      "|movieId|recommendations                                              |\n",
      "+-------+-------------------------------------------------------------+\n",
      "|148    |[[59134, 4.2925167], [7349, 4.2520046], [142811, 4.176842]]  |\n",
      "|463    |[[87426, 5.5767035], [10417, 4.989494], [149507, 4.893813]]  |\n",
      "|471    |[[138914, 5.1985846], [86599, 5.107135], [142811, 5.0901127]]|\n",
      "+-------+-------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.recommendForAllItems(3).show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "cSchema = StructType([StructField(\"movieID\", IntegerType())])\n",
    "\n",
    "test_list = [[111], [202], [225], [347], [488]]\n",
    "\n",
    "selected_df = spark.createDataFrame(test_list,schema=cSchema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|movieID|\n",
      "+-------+\n",
      "|    111|\n",
      "|    202|\n",
      "|    225|\n",
      "|    347|\n",
      "|    488|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------+\n",
      "|movieId|recommendations                                              |\n",
      "+-------+-------------------------------------------------------------+\n",
      "|225    |[[87426, 5.2278924], [10417, 4.9314265], [79224, 4.9270334]] |\n",
      "|111    |[[25160, 5.4074335], [142811, 5.396805], [108880, 5.3852315]]|\n",
      "|347    |[[142811, 5.450516], [67467, 5.2219796], [66426, 5.1274385]] |\n",
      "|202    |[[59134, 5.0535088], [142811, 5.01551], [148347, 4.864934]]  |\n",
      "|488    |[[142811, 5.040885], [59134, 4.8364477], [138914, 4.812961]] |\n",
      "+-------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.recommendForItemSubset(selected_df, 3).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering can be very effective in providing highly relevant recommendations. It scales well and can handle extremely large datasets. For collaborative filtering to operate optimally, it needs access to a large of amount of data. The more data, the better. As time progresses and ratings start to accumulate, recommendations become more and more accurate. Access to large datasets is often a problem during the early stages of implementation. One solution is to use content-based filtering in conjunction with collaborative filtering. Since content-based filtering doesn’t rely on user activity, it can immediately start providing recommendations, gradually increasing your dataset over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
